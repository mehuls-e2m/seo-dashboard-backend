# ğŸ” SEO Audit System

A comprehensive Python-based Technical & On-Page SEO Audit System that performs automated crawling, rule-based validation, and structured reporting.

## Features

### Technical SEO Audits
- âœ… Robots.txt compliance checking
- âœ… Sitemap discovery and validation
- âœ… Noindex/Nofollow detection
- âœ… Canonical tag validation
- âœ… Redirect chain analysis
- âœ… HTTPS and mixed content detection
- âœ… Structured data (JSON-LD, Microdata, RDFa) validation

### On-Page SEO Audits
- âœ… Title tag analysis (length, duplicates, templates)
- âœ… Meta description validation
- âœ… H1 tag checks (count, duplicates)
- âœ… Image alt text analysis
- âœ… Internal linking analysis (orphan pages, broken links)

### Scoring & Prioritization
- âœ… Rule-based scoring system (0-100)
- âœ… Severity-based issue prioritization (Critical/High/Medium/Low)
- âœ… Site-wide statistics and averages

### Output Formats
- âœ… JSON reports (detailed)
- âœ… CSV reports (summary)
- âœ… Console reports (human-readable)

## Installation

1. **Clone or download this repository**

2. **Install dependencies:**
```bash
pip install -r requirements.txt
```

3. **Install Playwright (optional, for JavaScript rendering):**
```bash
playwright install chromium
```

## Usage

Run the main script:

```bash
python main.py
```

The system will prompt you for:
1. **Website URL** - The URL to audit (e.g., `https://example.com`)
2. **Maximum pages** - Maximum number of pages to crawl (default: 50)

### Example Session

```
ğŸ” SEO AUDIT SYSTEM
================================================================================

ğŸŒ Enter website URL to audit: example.com
ğŸ“„ Enter maximum number of pages to crawl (default: 50): 100

ğŸš€ Starting audit for: https://example.com
ğŸ“Š Maximum pages to crawl: 100
```

## Output Files

The system generates two output files with timestamps:

- `seo_audit_YYYYMMDD_HHMMSS.json` - Detailed JSON report with all audit data
- `seo_audit_YYYYMMDD_HHMMSS.csv` - Summary CSV report for spreadsheet analysis

## Project Structure

```
.
â”œâ”€â”€ main.py                 # Main entry point
â”œâ”€â”€ crawler.py             # Async web crawler
â”œâ”€â”€ robots_sitemap.py      # Robots.txt and sitemap handling
â”œâ”€â”€ technical_audit.py     # Technical SEO audits
â”œâ”€â”€ onpage_audit.py        # On-page SEO audits
â”œâ”€â”€ rule_engine.py         # Scoring and prioritization
â”œâ”€â”€ output.py              # Report generation (CSV/JSON/Console)
â”œâ”€â”€ utils.py               # Utility functions
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md             # This file
```

## Features in Detail

### Crawling
- Respects robots.txt
- Rate limiting (2 requests/second per host)
- Retry logic with exponential backoff
- Concurrent crawling (configurable)
- Follows internal links only

### Logging
All operations are logged to console with emoji indicators:
- ğŸš€ Starting operations
- âœ… Success
- âš ï¸ Warnings
- âŒ Errors
- ğŸ“Š Statistics
- ğŸ” Auditing

## Scoring System

Each page receives a score from 0-100 based on:
- **Critical issues**: -40 points (e.g., noindex, not HTTPS)
- **High issues**: -20 points (e.g., missing title, canonical issues)
- **Medium issues**: -10 points (e.g., missing meta description, multiple H1s)
- **Low issues**: -5 points (e.g., missing alt text, duplicate content)

## Requirements

- Python 3.8+
- See `requirements.txt` for full dependency list

## Notes

- The crawler respects robots.txt and crawl-delay directives
- JavaScript rendering is optional (Playwright)
- All operations are logged to console for transparency
- The system is designed to be respectful and not overwhelm target servers

## License

This project is provided as-is for SEO auditing purposes.


